# ğŸ“ GUIA COMPLETO: TREINAMENTO DE AEON COM SEUS PADRÃ•ES

## ğŸ“ Estrutura de Pastas

```
training/
â”œâ”€â”€ raw_logs/              # Coloque seus arquivos aqui
â”‚   â”œâ”€â”€ README.md          # InstruÃ§Ãµes para exportar
â”‚   â”œâ”€â”€ chatgpt.json       # Exports de ChatGPT
â”‚   â”œâ”€â”€ claude.json        # Exports de Claude
â”‚   â”œâ”€â”€ discord.json       # Exports de Discord
â”‚   â””â”€â”€ conversation.log   # Seus logs locais
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ dados_training.txt    # ApÃ³s converter: VOCÃŠ/AEON
â”‚   â””â”€â”€ dados_training.jsonl  # Pronto para fine-tuning
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ aeon-offline-lora/    # Modelo treinado (Ollama)
â”‚   â””â”€â”€ aeon-online/          # Metadados (Groq)
â”œâ”€â”€ converter.py           # â­ Script de conversÃ£o universal
â”œâ”€â”€ extrair_padroes.py     # â­ Detecta seu estilo de fala
â””â”€â”€ TRAINING_GUIDE.md      # Este arquivo
```

---

## ğŸš€ PASSO 1: COLETAR LOGS

### OpÃ§Ã£o A: Seus logs do Aeon
- VocÃª jÃ¡ tem: `bagagem/temp/conversation.log`
- Copie para: `training/raw_logs/conversation.log`

### OpÃ§Ã£o B: ChatGPT
1. VÃ¡ em: https://chatgpt.com/c/...
2. Clique em **"Share"** â†’ **"Export as HTML/JSON"**
3. Salve em: `training/raw_logs/chatgpt_export.json`

### OpÃ§Ã£o C: Claude
1. Em Claude, clique na conversa
2. Menu â†’ **"Export conversation"**
3. Salve em: `training/raw_logs/claude_export.json`

### OpÃ§Ã£o D: Discord
1. Use um bot tipo Turing ou similar
2. Exporte JSON com estrutura: `[{"author": "vocÃª", "content": "...", "timestamp": "..."}, ...]`
3. Salve em: `training/raw_logs/discord_export.json`

---

## ğŸ”„ PASSO 2: CONVERTER LOGS

### Comando BÃ¡sico
```bash
cd training
python converter.py --source raw_logs/conversation.log --output processed/dados_training.txt
```

### Converter MÃºltiplos Arquivos
```bash
python converter.py --source raw_logs/ --output processed/dados_training.txt --jsonl --profile
```

### Flags:
- `--source FILE`: Arquivo ou pasta a converter
- `--output PATH`: Onde salvar (padrÃ£o: `processed/dados_training.txt`)
- `--jsonl`: TambÃ©m salva em formato JSONL (recomendado)
- `--profile`: Mostra anÃ¡lise de seus padrÃµes de linguagem

### Exemplo Completo:
```bash
python converter.py \
  --source raw_logs/ \
  --output processed/dados_combined.txt \
  --jsonl \
  --profile
```

**SaÃ­da esperada:**
```
ğŸ” Formato detectado: simple
ğŸ“‚ Processando: conversation.log
   âœ… 45 pares extraÃ­dos
ğŸ“‚ Processando: chatgpt.json
   âœ… 120 pares extraÃ­dos

==================================================
ğŸ“Š PERFIL DE LINGUAGEM DO USUÃRIO
==================================================

[PADRÃ•ES DO USUÃRIO]
- Usa gÃ­rias de dev: rodar, crashar, debug
- Tom: entusiasmado/direto
- Palavras frequentes: tipo, sistema, erro, rodar

==================================================

âœ¨ ConversÃ£o completa! 165 pares processados.
âœ… Salvo: processed/dados_combined.txt
âœ… Salvo (JSONL): processed/dados_combined.jsonl
```

---

## ğŸ¯ PASSO 3: EXTRAIR SEUS PADRÃ•ES DE LINGUAGEM

### Comando:
```bash
python extrair_padroes.py processed/dados_combined.txt
```

### O que faz:
1. **Detecta gÃ­rias de dev** que vocÃª usa (rodar, crashar, etc)
2. **Encontra expressÃµes favoritas** (tipo, nÃ©, sabe, etc)
3. **Identifica seu tom** (entusiasmado, irÃ´nico, investigativo, etc)
4. **Analisa estrutura** (perguntas frequentes? Comandos? ExplicaÃ§Ãµes longas?)
5. **Gera `CUSTOM_SYSTEM_PROMPT.md`** com suas caracterÃ­sticas

### SaÃ­da Esperada:
```
==================================================
ğŸ“Š PERFIL DE LINGUAGEM DETECTADO
==================================================

ğŸ”§ GÃRIAS DE DEV:
   - rodar: 12x
   - crashar: 8x
   - debug: 6x

ğŸ’¬ GÃRIAS/EXPRESSÃ•ES:
   - 'tipo': 45x
   - 'nÃ©': 32x
   - 'sabe': 28x

ğŸ“ PALAVRAS-CHAVE:
   - sistema, erro, cÃ³digo, funÃ§Ã£o, teste

ğŸ­ TOM PRINCIPAL: ENTUSIASMADO

ğŸ“ Comprimento mÃ©dio das mensagens: 87 caracteres

ğŸ˜€ Usa emojis com frequÃªncia: 15.3%

==================================================
âœ¨ System_prompt customizado salvo em: CUSTOM_SYSTEM_PROMPT.md
```

---

## ğŸ“ PASSO 4: REVISAR SEU SYSTEM_PROMPT CUSTOMIZADO

Abra `training/CUSTOM_SYSTEM_PROMPT.md` e veja:

```markdown
[IDENTIDADE]
VocÃª Ã© AEON...

[PERSONALIDADE BASE]
...

[GÃRIAS DO USUÃRIO]
VocÃª usa frequentemente: rodar, crashar, debug

[EXPRESSÃ•ES INFORMAIS]
Usa constantemente: tipo, nÃ©, sabe

[TOM]
Seu tom Ã©: entusiasmado. Responda de forma similar.

[ESTILO]
O usuÃ¡rio faz muitas perguntas. Seja conversacional.
O usuÃ¡rio dÃ¡ muito comando. Responda executivo, sem blablabla.

[DIRETRIZES TÃ‰CNICAS]
MODO 1: AÃ‡ÃƒO (se pedir algo que exija interaÃ§Ã£o)
Responda EXATAMENTE com: {"tool": "Modulo.funcao", "param": "valor"}

MODO 2: CONVERSA (papo/filosofia)
Responda com TEXTO PURO. MÃ¡x 2 parÃ¡grafos. Seja conciso.
```

---

## ğŸ§  PASSO 5: ESCOLHER MÃ‰TODO DE TREINAMENTO

VocÃª tem 3 opÃ§Ãµes. VocÃª pode usar as 3 em paralelo!

### OPÃ‡ÃƒO 1ï¸âƒ£: Groq (Cloud - Recomendado para comeÃ§ar)
**Vantagem:** RÃ¡pido, sem instalar nada, resultados em horas.

#### Comandos:
```bash
# 1. Converter para formato Groq
python converter.py --source processed/dados_combined.txt --jsonl

# 2. Fazer fine-tune na Groq API (via CLI ou Python)
pip install groq

# Depois copie o script que vou fornecer:
python groq_finetuning.py --data processed/dados_combined.jsonl --model llama-3.3-70b

# 3. Integrar em core/brain.py
# Seus modelos ficarÃ£o em: https://console.groq.com/keys
```

#### Resultado:
- Modelo treinado na nuvem
- DisponÃ­vel via API Groq
- Chamadas normais em `core/brain.py` (transparente)

---

### OPÃ‡ÃƒO 2ï¸âƒ£: Ollama (Local + LoRA - Bom balanÃ§o)
**Vantagem:** Corre local, privado, sem API keys.

#### Comandos:
```bash
# 1. Instalar Ollama se nÃ£o tiver
# https://ollama.ai

# 2. Converter dados
python converter.py --source processed/dados_combined.txt --jsonl

# 3. Usar LLaMA Factory para fine-tune
pip install llama-factory

# Depois (vou fornecer config):
llamafactory-cli train models/ollama_lora_config.yaml

# 4. Exportar LoRA para Ollama
# Usar script de conversÃ£o que vou fornecer

# 5. Testar local
ollama run aeon-personalized
```

#### Resultado:
- Modelo rodando localmente
- No seu computador, sem internet
- Integra em `core/brain.py` como fallback

---

### OPÃ‡ÃƒO 3ï¸âƒ£: Unsloth (Mais rÃ¡pido ainda)
**Vantagem:** Treinamento ultrarÃ¡pido, otimizado, excelente qualidade.

#### Comandos:
```bash
# 1. Instalar
pip install unsloth

# 2. Converter dados
python converter.py --source processed/dados_combined.txt --jsonl

# 3. Treinar (script que vou fornecer)
python unsloth_train.py --data processed/dados_combined.jsonl

# 4. Exportar modelo
# Pode virar LoRA para Ollama ou arquivo completo

# 5. Integrar em core/brain.py
```

#### Resultado:
- Modelo treinado em 15-30 min (vs horas em outros)
- Qualidade excelente
- Funciona com Groq ou Ollama

---

## ğŸ“‹ CHECKLIST COMPLETO

### Sem Fine-Tuning:
- [x] Aeon conversa naturalmente âœ…
- [x] MÃ³dulos funcionam âœ…
- [x] Brain usa Groq+Ollama âœ…

### Com Fine-Tuning:
- [ ] **Copiar logs para** `training/raw_logs/`
- [ ] **Rodar converter.py** â†’ gera `dados_training.jsonl`
- [ ] **Rodar extrair_padroes.py** â†’ gera `CUSTOM_SYSTEM_PROMPT.md`
- [ ] **Revisar** `CUSTOM_SYSTEM_PROMPT.md`
- [ ] **Escolher 1 opÃ§Ã£o** (Groq / Ollama / Unsloth)
- [ ] **Treinar** (15 min a 2 horas)
- [ ] **Exportar** modelo
- [ ] **Integrar em** `core/brain.py`
- [ ] **Testar** "OlÃ¡ Aeon!" â†’ responde com seu estilo!

---

## ğŸ”— ARQUIVOS RELACIONADOS

1. **converter.py** - Transforma logs de qualquer fonte em VOCÃŠ/AEON
2. **extrair_padroes.py** - Detecta seu estilo + gera system_prompt customizado
3. **core/brain.py** - Integra os modelos treinados

---

## âš ï¸ TROUBLESHOOTING

### "Erro ao ler arquivo"
- Verifique encoding (deve ser UTF-8)
- Se for `.json`, valide em: https://jsonlint.com/

### "Nenhum par encontrado"
- Seu arquivo pode estar vazio ou em formato nÃ£o reconhecido
- Tente converter manualmente para VOCÃŠ/AEON primeiro

### "JSONL invÃ¡lido"
- Cada linha deve ser um JSON completo
- Use: `python -m json.tool seu_arquivo.jsonl` para validar

### "Modelo nÃ£o carrega"
- Verifique se Ollama estÃ¡ rodando: `ollama list`
- Se Groq, verifique `GROQ_KEY` em `.env`

---

## ğŸ PRÃ“XIMOS PASSOS

ApÃ³s treinar:

1. **Teste o modelo:**
   ```bash
   cd d:\Dev\Aeon\Aeon
   python main.py
   # Fale: "OlÃ¡ Aeon! Como tÃ¡?"
   # Esperado: Resposta em seu estilo (gÃ­rias, tom, etc)
   ```

2. **Refine mais:**
   - Cole suas conversas atuais em `raw_logs/`
   - Reconverta e retreine a cada semana

3. **Personalize ainda mais:**
   - Edite `CUSTOM_SYSTEM_PROMPT.md` manualmente
   - Adicione diretrizes que sÃ³ vocÃª sabe

---

## ğŸ“ DÃšVIDAS?

**Q: Posso usar todas as 3 opÃ§Ãµes?**
A: Sim! Teste uma de cada. Veja qual responde melhor no seu caso.

**Q: Quanto tempo leva?**
A: Groq: 1-2 horas (cloud). Ollama: 2-4 horas (local). Unsloth: 15-30 min (rÃ¡pido).

**Q: Preciso de GPU?**
A: Unsloth e Ollama sÃ£o mais rÃ¡pidos com GPU, mas rodam em CPU tambÃ©m.

**Q: E se tiver poucos dados?**
A: MÃ­nimo 20 pares. Ideal: 100+. Quanto mais, melhor a personalizaÃ§Ã£o.

**Q: Aeon vai esquecer dos mÃ³dulos?**
A: NÃ£o! O fine-tuning Ã© sÃ³ para conversa. MÃ³dulos mantÃªm funcionamento normal.

---

## ğŸš€ Comece Agora!

```bash
cd training
python converter.py --source raw_logs/ --jsonl --profile
python extrair_padroes.py processed/dados_training.txt
```

Depois me mostra os resultados! ğŸ¯
