# -*- coding: utf-8 -*-
"""
SCRIPT DE TREINAMENTO LOCAL COM UNSLOTH
Este script treina um adaptador LoRA no seu PC usando a biblioteca Unsloth,
que √© super r√°pida e otimizada para GPUs de consumidor.

O resultado ser√° um "patch" de personaliza√ß√£o para um modelo de linguagem grande.
"""

# 1. Importa√ß√µes e checagem de ambiente
import torch
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset
import os

# Verifica se tem GPU NVIDIA
if not torch.cuda.is_available():
    raise SystemError("GPU NVIDIA n√£o detectada. Unsloth requer CUDA.")

print("‚úÖ GPU NVIDIA detectada. Iniciando o processo de treinamento com Unsloth.")

# 2. Carregar o modelo base
# Usamos um modelo da biblioteca Unsloth j√° otimizado.
# Llama-3 8B √© um excelente ponto de partida.
max_seq_length = 2048  # Comprimento m√°ximo da sequ√™ncia
dtype = None  # None para detec√ß√£o autom√°tica
load_in_4bit = True  # Ativa a quantiza√ß√£o de 4 bits (QLoRA) para economizar mem√≥ria

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3-8b-bnb-4bit",
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
)
print("‚úÖ Modelo base (Llama-3 8B) carregado.")

# 3. Configurar o modelo para treinamento LoRA
# Adicionamos "adaptadores" LoRA ao modelo. Apenas esses adaptadores ser√£o treinados.
model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # Rank (tamanho) dos adaptadores. 16 √© um bom valor.
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing=True,
    random_state=3407,
)
print("‚úÖ Adaptadores LoRA adicionados ao modelo.")

# 4. Prepara√ß√£o do Dataset
# Carregamos o arquivo JSONL que voc√™ gerou anteriormente.
dataset_path = "processed/dados_training.jsonl"
if not os.path.exists(dataset_path):
    raise FileNotFoundError(f"Arquivo de dataset n√£o encontrado em: {dataset_path}. "
                            "Certifique-se de que o script converter.py foi executado com sucesso.")

dataset = load_dataset("json", data_files={"train": dataset_path})['train']

# O Unsloth precisa que o dataset tenha uma coluna "text".
# Vamos criar uma fun√ß√£o para formatar nossas conversas nesse padr√£o.
alpaca_prompt = """Abaixo est√° uma instru√ß√£o que descreve uma tarefa. Escreva uma resposta que complete adequadamente o pedido.

### Instru√ß√£o:
{}

### Resposta:
{} """

def formatting_func(examples):
    users = examples["user"]
    assistants = examples["assistant"]
    texts = []
    for user, assistant in zip(users, assistants):
        text = alpaca_prompt.format(user, assistant)
        texts.append(text)
    return {"text": texts}

dataset = dataset.map(formatting_func, batched=True)
print("‚úÖ Dataset formatado e pronto para o treinamento.")

# 5. Configurar e Iniciar o Treinamento
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    packing=False,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        max_steps=100,  # Aumente para 100-200 para um treino melhor. 60 √© para teste r√°pido.
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        output_dir="outputs",
        optim="paged_adamw_8bit",
        seed=3407,
    ),
)

print("\nüöÄ INICIANDO TREINAMENTO... (Isso pode levar de 5 a 20 minutos)")
trainer.train()
print("üéâ Treinamento conclu√≠do!")

# 6. Salvar o Adaptador LoRA
output_dir = "models/aeon_lora_adapter"
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
print(f"\n‚úÖ Adaptador LoRA salvo em: {output_dir}")
print("Este √© o 'patch' de personaliza√ß√£o do seu Aeon.")

# Informa√ß√µes finais
print("\n---")
print("PR√ìXIMO PASSO: Fazer o upload deste adaptador para a Groq.")
print("Os arquivos que voc√™ precisa est√£o na pasta 'models/aeon_lora_adapter'.")
print("Execute o pr√≥ximo script que vou fornecer para completar o processo.")
